---
title: "The Data of Pitchfork — Album Analysis"
author: "Matt Wolfinger"
date: "03/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load the tidyverse
library(tidyverse)

```

## Introducing the data

In this report, data from Nolan Conaway on Kaggle entitled "18,393 Pitchfork Reviews" are used to analyze and examine the many nuances present in reviewing an album of music for an online publication. The data catalogues over 18,000 music reviews from music-centric online publication Pitchfork. Conaway scraped these reviews through Python. The scrape pulled data of Pitchfork album reviews for the timeframe **January 5, 1999 through January 8, 2017**, leaving us with nearly two decades of data to work with.

The original was in an sqlite file format, which I eventually converted into a .csv file through the DB Browser for SQLite application.

I imported those files into Google Sheets and manually compiled them into one unified table, which I labeled "pitchfork." From there, I saved it as a .csv file and imported it into RStudio. A tedious process to be sure, but one that I was determined to take on due to my personal love for music and the value I saw in the potential of the data set in addition to the sequential analysis. 

In the data set, each row corresponds to a single album of music. For each album, we examine the following variables:

* reviewid: the id number of the review
* title: the title of the album
* artist: the artist who produced the album
* genre: the genre of music the album falls under, dictated by Pitchfork themselves
* label: the music label the album was released under (includes "self-published")
* score: the final precise scoring given to the album (0.0 - 10.0)
* best_new_music: noted in binary code, with 0 indicating no "Best New Music" label included and 1 indicating a "Best New Music" label inclusion.
* author: the person who wrote the piece
* author_type: the relation/role of the author to the Pitchfork publication
* pub_date: the numeric breakdown of the date of publication (0000-00-00 format)
* pub_weekday: the day of the week the review was published, indicated by a 0 - 6 value (where 0 represents Monday and 6 represents Sunday)
* pub_day: the day of the month the review was published
* pub_month: the month of the year the review was published
* pub_year: the year the review was published (between 1997 - 2017)
* year: the year of the reviewed album's release

For the sake of transparency, note that a single column entitled "url" was removed from the raw data set for the sake of this analysis, as the urls of the articles do not pertain to the analysis at hand. Another column entitled "content" was removed from the raw data set, which contained the actual text of the reviews. In a smaller set of data, the content of the articles would have been interesting to evaluate. For the sake of not bricking my laptop and considerably increasing load times, I decided to opt out of including it this time. That, and Google Sheets couldn't handle importing it for compiling into one .csv file. So actually—I blame Google.

Additionally, Rows 18395 - 22680 were omitted. The data contained in these rows are unclear and incomplete. According to Conaway, he catalogued "18,393" reviews. As such, the data that are analyzed should only pertain to all rows up until 18,394 (with the header of each column acting as the first row).


```{r load-data, results = "hide"}
# Load the data from the file
pitchfork.rev <- read_csv("./data/pitchfork.csv")

# Take a look at what we loaded
View(pitchfork.rev)

```

Everything seems to have transitioned over smoothly! Now comes the interesting part.

# Analyzing the data

We can now perform some basic analysis on the data loaded. The sections that follow explain that analysis.

## Number of reviews per genre

With over 18,000 reviews in the data set, one of the first questions I had concerned the breakdown of genre. Said breakdown can be examined below.

```{r reveiws-per-genre}

# Identify total reviews per genre and see the most at the top
pitchfork.rev %>%
  group_by(genre) %>%
  summarize(total = n()) %>% arrange(desc(total))
```

And there we have it! Rock is the winner, having over *double* the reviews as the second-place contender, electronic. 

The third most popular genre is actually the *absence* of genre. Sure, it could have something to do with the reviewer wanting to say it transcends whatever genre exists, but I personally think it may have something to do with the semi-restricting amount of nine total genres that it seems nearly 2000 albums didn't fit into. We'll take care of those reviews a bit later.

## Average score per genre

So here's where the latter part of this analysis comes in. The second observation I want to include in this data visualization concerns the average score of each genre.

```{r score-per-genre}

# Calculate the average review score for each genre

pitchfork.rev %>% group_by(genre) %>%
  summarize(average.score = mean(score)) %>% arrange(desc(average.score))
```

Metal is the highest rated genre, with rap and experimental close behind. But *wait* you may be asking, wouldn't rock be subject to a much lower score as a result of having the most reviews by a long-shot? I'm so glad you asked! That's exactly what I'm going to compare.

# Visualizing the data 

For the rest of our time spent together, we'll be exploring the previous observations through the creation of in-depth data visualization to tell stories with the information we've obtained.

## Ratings by genre

Right now, we're going to create a data visualization that compares the genre of review to its average score (y-axis), using the context of the number of reviews (x-axis) to see if there's a correlation.

```{r ratings-by-genre}

#After omitting the values with no genre assigned, we'll calculate the average review score for each and map them on a scatter plot with a narrowed y-axis. If you're paying attention, you'll notice we use the same tactics as the previous section.

pitchfork.rev %>% group_by(genre) %>%
  summarize(total = n(), average.score = mean(score)) %>%
  na.omit(genre) %>%
  ggplot(mapping = aes(x=total, y= average.score)) +
  geom_point(mapping = aes(), size = 2, color = "darkred") +
  ylim(6.75,7.25) +
  labs(title = "Average Pitchfork Ratings by Genre", x = "Total Reviews", y = "Average Review Score") +
  theme_minimal()

ggsave("ratings.by.genre.png")

```
The narrowing of the y-axis turned out pretty well visually—it gives the data room to breathe while still emphasizing how close the scores are in their averages.

As it turns out, there really isn't much of a difference in the average review score per genre, despite some being far more prevalent in Pitchfork's catalogue. That's a testament to the impartial nature of their most prominent critics. Just as I thought, metal—the genre with the highest average—is one of the newer ones. I'm sure that'll level out with time, as is the case with rock.

## Timeline of Critic and Average Review Score

Next thing I'm looking at concerns a theory that I wanted to evaluate concerning the amount of articles a critic has written and their overall average scores over time. Do critics that have been writing for Pitchfork longer become more cynical with their scores?

```{r average-author-score-timeline}

#create a data set that stores the number of times each critic appears, and filter critics so only those who have written consistently are shown, to simplify data. I opted to only include critics who have written at least 25 reviews.

author.count <- pitchfork.rev %>% group_by(author) %>% summarize(count = n(), average.score = mean(score))%>%
filter(count >= 25)

#Now let's create the actual visualization.

author.count %>% group_by(author) %>%
  ggplot(mapping = aes(x= count, y= average.score)) +
  geom_point(mapping = aes(), size = 1, color = "indianred4") +
  geom_smooth(mapping = aes(), color = "seagreen", fill = "gray85") +
  labs(title = "Average Pitchfork Ratings by Critic", subtitle = "who have written at least 25 reviews", x = "Total Reviews", y = "Average Review Score") +
  scale_x_continuous(breaks = c(25,250,500,750), limits = c(25,850)) +
  theme_minimal()

ggsave("ratingsbycritic.png")

```
I've opted to use geom_smooth for this visualization in order to aid the narrative and intended takeaway. It seems as though those with a longer history stay pretty consistent with giving average scores, and that there's no correlation between the number of reviews and the average review score of a critic.

## Scores vs Best New Music

The "Best New Music" title is put very rarely by Pitchfork critics on an album review. It's the website's equivalent of getting a gold star or added to a favorites list. There's no real answer as to what classifies something as good enough to receive the title, but I have a working theory that there may be some sort of correlation between the scores that critics give to the albums and the prevalence of "Best New Music" ratings. It may be possible that the distribution of the "Best New Music" label becomes more common with higher scores. Since this practice began in 2003, we'll be sorting the data accordingly.

```{r scores-vs-bnm}

#A histogram is going to be our best bet to display this data. The visualization will utilize two separate histograms, each with their own layers, to illustrate the distribution. Let's go a step further and shift the transparency using "alpha."

new.ob <- pitchfork.rev %>% filter(year > 2002) %>%
  select(score, best_new_music)
  ggplot(mapping=aes(x = score)) +
  geom_histogram(data = filter(new.ob, best_new_music == 0), binwidth=.1, fill = "olivedrab", alpha = 0.75) +
  geom_histogram(data = filter(new.ob, best_new_music == 1), binwidth=.1, fill = "orangered4", alpha = 0.6) +
  labs(title = "Relationship Between Scores and 'Best New Music' Tags", subtitle = "album review scores and the distribution of Pitchfork's elusive label", x = "Score", y = "Number of Reviews", fill = "Best New Music") +
  theme_minimal()
  

ggsave("best.new.music.png")
```
As you can see clearly illustrated in that visualization, there does appear to be some sort of correlation between the prevalence of distribution for the "Best New Music" tag as the score the album receives increases. I'm hesitant to declare any sort of causation between the two because, well, that's a day one data mistake.

#What's Next

Moving forward, it'd be interesting to see what that tiny speck of perfect 10s without "Best New Music" labels are (I'm guessing they may have something to do with retrospective reviews of posthumous artists). I'm also going to go back and find the articles written by of some of the outlier critics in the critic scatter plot to link out to them. Checking out some of those pessimistic reviewers may make for a humorous time.
